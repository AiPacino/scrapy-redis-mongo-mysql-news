#coding=utf-8
from scrapy import cmdline
# cmdline.execute("scrapy crawl renminribao".split())
# cmdline.execute("scrapy crawl guangmingribao".split())
# cmdline.execute("scrapy crawl zhongguoxinwen".split())
# cmdline.execute("scrapy crawl souhuxinwen".split())
# cmdline.execute("scrapy crawl xinlang".split())
# cmdline.execute("scrapy crawl renmin".split())
# cmdline.execute("scrapy crawl tiantiankuaibao".split())
# cmdline.execute("scrapy crawl jinri".split())
# cmdline.execute("scrapy crawl mingbaoxinwen".split())
# cmdline.execute("scrapy crawl sanlinews".split())
# cmdline.execute("scrapy crawl wyxinwen".split())
# cmdline.execute("scrapy crawl deguozhisheng".split())
# cmdline.execute("scrapy crawl hanlianshe".split())
# cmdline.execute("scrapy crawl shikuang".split())
# cmdline.execute("scrapy crawl tengxunxinwen".split())
# cmdline.execute("scrapy crawl tengxun".split())
# cmdline.execute("scrapy crawl fenghuang".split())
# cmdline.execute("scrapy crawl dongfangtoutiao".split())
# cmdline.execute("scrapy crawl xianggang01".split())
# cmdline.execute("scrapy crawl xianggang02".split())
# cmdline.execute("scrapy crawl taiwanyizhoukan".split())
# cmdline.execute("scrapy crawl xianggangyizhoukan".split())
# cmdline.execute("scrapy crawl zhongshidianzibao".split())
# cmdline.execute("scrapy crawl zhongshidianzi".split())
# cmdline.execute("scrapy crawl wuxianxinwen".split())
# cmdline.execute("scrapy crawl shangyouxinwen".split())
# cmdline.execute("scrapy crawl lichangxinwen".split())
# cmdline.execute("scrapy crawl dajiyuan".split())
# cmdline.execute("scrapy crawl zhongguojinwen".split())
# cmdline.execute("scrapy crawl toutiaocaijing".split())
# cmdline.execute("scrapy crawl lianhezaobao".split())
# cmdline.execute("scrapy crawl dongsenxinwen".split())
# cmdline.execute("scrapy crawl lutouxinwen".split())
# cmdline.execute("scrapy crawl shijieribao".split())
# cmdline.execute("scrapy crawl dongtaiwang".split())
# cmdline.execute("scrapy crawl zhangshangliuyuan".split())
# cmdline.execute("scrapy crawl duoweixinwen".split())
# cmdline.execute("scrapy crawl dongwang".split())
# cmdline.execute("scrapy crawl yidianzixun".split())
# cmdline.execute("scrapy crawl jinritoutiao".split())
# cmdline.execute("scrapy crawl rmrbdata".split())
# cmdline.execute("scrapy crawl renminnews".split())
# cmdline.execute("scrapy crawl zhongyangshe".split())
# cmdline.execute("scrapy crawl baiduxinwen".split())
# cmdline.execute("scrapy crawl baidu".split())
# cmdline.execute("scrapy crawl hanlianshe".split())
# cmdline.execute("scrapy crawl meiguozhiyin".split())
# cmdline.execute("scrapy crawl meiguozhiyinzhongwenwang".split())
# cmdline.execute("scrapy crawl ziyouyazhou".split())
# cmdline.execute("scrapy crawl jingjixueren".split())
# cmdline.execute("scrapy crawl ftyingwenwang".split())
# cmdline.execute("scrapy crawl pengbo".split())
# cmdline.execute("scrapy crawl xinhuashe".split())
# cmdline.execute("scrapy crawl rijingxinwen".split())
# cmdline.execute("scrapy crawl eluosiweixing".split())
# cmdline.execute("scrapy crawl wyxinwen".split())
# cmdline.execute("scrapy crawl duanchuanmei".split())
# cmdline.execute("scrapy crawl shangguanxinwen".split())
# cmdline.execute("scrapy crawl shangyouxinwen".split())
# cmdline.execute("scrapy crawl nanfangzhoumo".split())
# cmdline.execute("scrapy crawl ziyouyazhou1".split())
# cmdline.execute("scrapy crawl radiofreeasia".split())
# cmdline.execute("scrapy crawl tianxiazazhimeiribao".split())
# cmdline.execute("scrapy crawl mingbao".split())
# cmdline.execute("scrapy crawl jiaohuidianxinwen".split())
# cmdline.execute("scrapy crawl duanchuanmei".split())
# cmdline.execute("scrapy crawl dongtaiwang".split())
# cmdline.execute("scrapy crawl cnbc".split())
# cmdline.execute("scrapy crawl meiguozhiyinzhongwen".split())
# cmdline.execute("scrapy crawl nbcnews".split())
cmdline.execute("scrapy crawl zhongguoxinwen".split())
# cmdline.execute("scrapy crawl yindushibao".split())
# cmdline.execute("scrapy crawl xiangangxinwen".split())
# cmdline.execute("scrapy crawl afp".split())
# cmdline.execute("scrapy crawl afpnews".split())
# cmdline.execute("scrapy crawl youbao".split())
# cmdline.execute("scrapy crawl chaorixinwen".split())
# cmdline.execute("scrapy crawl huaerjieyingwen".split())
# cmdline.execute("scrapy crawl wyxinwen".split())
# cmdline.execute("scrapy crawl huanqiushibao".split())
# cmdline.execute("scrapy crawl nanhuazaobao".split())
# cmdline.execute("scrapy crawl guardian".split())
# cmdline.execute("scrapy crawl eluosi".split())
# cmdline.execute("scrapy crawl cnbcnews".split())
cmdline.execute("scrapy crawl yindushibaoyingwen".split())
cmdline.execute("scrapy crawl niuyueshibao".split())
cmdline.execute("scrapy crawl huaerjieyingwenwang".split())
# cmdline.execute("scrapy crawl jingjixueren".split())
# import time
# import datetime
# time='26 Jun 2018 13:14:34'
# time_format=datetime.datetime.strptime(time,'%d %B %Y %H:%M:%S')
# print time_format
# import requests
# data = requests.get('http://119.23.19.90:5000/news?category=business/list')
# print data.content

# import time
# t = '1528769010'
# print time.time()
# Ttime = int(round(time.time() * 1000))
# a = '2018-06-12 10:03:30'
# timeArray = time.strptime(a, "%Y-%m-%d %H:%M:%S")
# timeStamp = int(time.mktime(timeArray))
# print timeStamp
# publishedDate = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(float(t)))
# print publishedDate
# time1 = time.strftime("%Y-%m-%d %H:%M:%S")
# time2 = time.strptime(time1, "%Y-%m-%d %H:%M:%S")
# time3 = int(time.mktime(time2))
# print time3
# t = '%f'%time.time()
# import re
# print re.findall('\d+','%f'%time.time())[0]
# print time.localtime(time.time())
# import time
# print  int(round(time.time() * 1000))